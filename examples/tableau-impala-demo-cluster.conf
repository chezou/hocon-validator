include file("your-aws-info.conf")

name: tableau-demo-cluster
provider {
    type: aws
    accessKeyId: ${?AWS_ACCESS_KEY_ID}
    secretAccessKey: ${?AWS_SECRET_ACCESS_KEY}
    region: ${?AWS_REGION}
    subnetId: ${?AWS_SUBNET_ID}
    securityGroupsIds: ${?AWS_SECURITY_GROUP}
}
ssh {
    username: ${?OS_USERNAME}
    privateKey: ${?KEY_PAIR}
}
common-instance-properties {
    image: ${?AWS_AMI}
    tags {
        owner: ${?INSTANCE_OWNER}
    }
}
instances {
    t2l : ${common-instance-properties}{
        type: t2.large #vCPU 2, RAM 8G
    }
    t2xl : ${common-instance-properties}{
        type: t2.xlarge #vCPU 4, RAM 16G
    }
    r3xl : ${common-instance-properties} {
        type: r3.xlarge #vCPU 4, RAM 30.5G, SSD 80Gx1
    }
}
cloudera-manager {
    instance: ${instances.t2xl} {
        instanceNamePrefix: ${?INSTANCE_NAME_PREFIX}"-cm"
        tags {
            application: "Cloudera Manager 5"
        }
    }
    enableEnterpriseTrial: true

    repository: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5.12/"
    repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/RPM-GPG-KEY-cloudera"

    configs {
        CLOUDERA_MANAGER {
            custom_banner_html: "Tableau demo cluster managed by Cloudera Director"
        }
        NAVIGATORMETASERVER { 
            # for Navigator Demo
            navigator_safety_valve: "nav.extractor.poll_period=10"
        }
    }
}
cluster {
    products {
        CDH: 5
        KUDU: 1.4.0
    }
    configs {
        HDFS {
            core_site_safety_valve: "<property><name>fs.s3a.access.key</name><value>"${?AWS_ACCESS_KEY_ID}"</value></property><property><name>fs.s3a.secret.key</name><value>"${?AWS_SECRET_ACCESS_KEY}"</value></property>"
        }
    }
    parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/5.12/","http://archive.cloudera.com/kudu/parcels/5.12/"]
    services: [ZOOKEEPER, HDFS, YARN, HIVE, IMPALA, OOZIE, HUE, KUDU]

    master {
        count: 1

        instance: ${instances.t2xl} {
            instanceNamePrefix: ${?INSTANCE_NAME_PREFIX}"-master"
            tags {
                group: master
            }
        }

        roles {
            ZOOKEEPER: [SERVER]
            HDFS: [NAMENODE,SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER,JOBHISTORY]
            OOZIE: [OOZIE_SERVER]
            HUE: [HUE_SERVER]
            HIVE: [HIVEMETASTORE,HIVESERVER2]
            IMPALA: [STATESTORE,CATALOGSERVER]
            KUDU: [KUDU_MASTER]
        }
        configs {
            HDFS {
                NAMENODE {
                    dfs_name_dir_list: "/data0/nn"
                }
                SECONDARYNAMENODE {
                    fs_checkpoint_dir_list: "/data0/snn"
                }
            }
            KUDU {
	            KUDU_MASTER {
	                # The master rarely performs IO. If fs_data_dirs is unset, it will
	                # use the same directory as fs_wal_dir
	                fs_wal_dir: "/data0/kudu/masterwal"
	                fs_data_dirs: "/data0/kudu/master"
	            }
            }
        }
    }

    worker {
        count: ${?WORKER_NODE_NUM}
        minCount: ${?WORKER_NODE_NUM}
        instance: ${instances.r3xl} {
            instanceNamePrefix: ${?INSTANCE_NAME_PREFIX}"-worker"
            tags {
                group: worker
            }
        }

        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            IMPALA: [IMPALAD]
            KUDU: [KUDU_TSERVER]
        }
        configs {
            HDFS {
                DATANODE {
                    dfs_data_dir_list: "/data0/dn"
                }
            }
            KUDU {
                KUDU_TSERVER {
                    # Set fs_wal_dir to an SSD drive (if exists) for better performance.
                    # Set fs_data_dirs to a comma-separated string containing all remaining
                    # disk drives, solid state or otherwise.
                    # If there are multiple drives in the machine, it's best to ensure that
                    # the WAL directory is not located on the same disk as a tserver data
                    # directory.
                    fs_wal_dir: "/data0/kudu/tabletwal"
                    fs_data_dirs: "/data0/kudu/tablet"

                    #If you really use Kudu for demo,
                    #set appropriate memory_limit_hard_bytes for better performance.
                    #memory_limit_hard_bytes: 17179869184 #16GiB
                    #block_cache_capacity_mb: 2048 #2GiB
                }
            }
            IMPALA {
                IMPALAD {
                    impalad_memory_limit: 17179869184 #16GiB
                }
            }
        }
    }

    instancePostCreateScripts: ["""#!/bin/sh
set -ex
echo "${HOSTNAME}: instancePostCreateScripts"
exit 0
    """]

    postCreateScripts: ["""#!/bin/sh
set -ex
echo "${HOSTNAME}: postCreateScripts"
exit 0
    """]
}
